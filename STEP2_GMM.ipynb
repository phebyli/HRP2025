{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddc1d82-38be-4bc6-99bf-f7e41f312e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Selecting features...\n",
      "Scaling features...\n",
      "Reducing dimensionality...\n",
      "Optimizing GMM clustering...\n",
      "Analyzing top features...\n",
      "Creating 2D visualization...\n",
      "Generating visualizations...\n",
      "Saving results...\n",
      "Optimal number of clusters: 2\n",
      "Best silhouette score: 0.76\n",
      "Total statistically significant features: 412\n",
      "Most important feature: AP_log-sigma-3-0-mm-3D_firstorder_Minimum (importance=0.682)\n",
      "Visualizations saved to ./figures\n",
      "Results saved to ./results\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Configuration Constants\n",
    "CONFIG = {\n",
    "    'data': {\n",
    "        'filepath': './results/radiomics.csv',\n",
    "        'index_col': 0\n",
    "    },\n",
    "    'feature_selection': {\n",
    "        'var_threshold': 0.02,\n",
    "        'corr_threshold': 0.9\n",
    "    },\n",
    "    'dimension_reduction': {\n",
    "        'n_components': 10,\n",
    "        'n_neighbors': 10,\n",
    "        'min_dist': 0.02,\n",
    "        'metric': 'cosine',\n",
    "        'random_state': 42,\n",
    "        'final_components': 2\n",
    "    },\n",
    "    'clustering': {\n",
    "        'max_clusters': 5,\n",
    "        'covariance_type': 'full',\n",
    "        'random_state': 42\n",
    "    },\n",
    "    'analysis': {\n",
    "        'top_features': 50\n",
    "    },\n",
    "    'visualization': {\n",
    "        'figsize': (12, 8),\n",
    "        'cmap': 'jet',\n",
    "        'heatmap_figsize': (16, 12),\n",
    "        'dpi': 300\n",
    "    },\n",
    "    'output': {\n",
    "        'results_dir': './results',\n",
    "        'figures_dir': './figures'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(CONFIG['output']['results_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['output']['figures_dir'], exist_ok=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(CONFIG['clustering']['random_state'])\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    @staticmethod\n",
    "    def load_data():\n",
    "        df = pd.read_csv(CONFIG['data']['filepath'], index_col=CONFIG['data']['index_col'])\n",
    "        return df.index.values, df.values, df.columns.tolist()\n",
    "\n",
    "    @staticmethod\n",
    "    def select_features(features, feature_names):\n",
    "        selector = VarianceThreshold(threshold=CONFIG['feature_selection']['var_threshold'])\n",
    "        features_high_var = selector.fit_transform(features)\n",
    "        selected_indices = selector.get_support(indices=True)\n",
    "        selected_features = [feature_names[i] for i in selected_indices]\n",
    "\n",
    "        df = pd.DataFrame(features_high_var, columns=selected_features)\n",
    "        corr_matrix = df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > CONFIG['feature_selection']['corr_threshold'])]\n",
    "\n",
    "        var_values = df.var()\n",
    "        to_keep = []\n",
    "        for col in to_drop:\n",
    "            correlated_cols = upper.index[upper[col] > CONFIG['feature_selection']['corr_threshold']].tolist()\n",
    "            correlated_cols.append(col)\n",
    "            best_feature = var_values[correlated_cols].idxmax()\n",
    "            if best_feature not in to_keep:\n",
    "                to_keep.append(best_feature)\n",
    "\n",
    "        final_features = [f for f in selected_features if f not in to_drop or f in to_keep]\n",
    "        return df[final_features].values, final_features\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_features(features):\n",
    "        return StandardScaler().fit_transform(features)\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_dimension(features):\n",
    "        reducer = umap.UMAP(\n",
    "            n_components=CONFIG['dimension_reduction']['n_components'],\n",
    "            n_neighbors=CONFIG['dimension_reduction']['n_neighbors'],\n",
    "            min_dist=CONFIG['dimension_reduction']['min_dist'],\n",
    "            metric=CONFIG['dimension_reduction']['metric'],\n",
    "            random_state=CONFIG['dimension_reduction']['random_state']\n",
    "        )\n",
    "        return reducer.fit_transform(features)\n",
    "\n",
    "\n",
    "class ClusterAnalyzer:\n",
    "    @staticmethod\n",
    "    def optimize_gmm(features):\n",
    "        best_score, best_gmm, best_n = -1, None, 2\n",
    "        results = []\n",
    "        for n in range(2, CONFIG['clustering']['max_clusters'] + 1):\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=n,\n",
    "                covariance_type=CONFIG['clustering']['covariance_type'],\n",
    "                random_state=CONFIG['clustering']['random_state']\n",
    "            )\n",
    "            clusters = gmm.fit_predict(features)\n",
    "            metrics = {\n",
    "                'n_clusters': n,\n",
    "                'silhouette': round(silhouette_score(features, clusters), 2),\n",
    "                'calinski_harabasz': round(calinski_harabasz_score(features, clusters), 2),\n",
    "                'davies_bouldin': round(davies_bouldin_score(features, clusters), 2)\n",
    "            }\n",
    "            results.append(metrics)\n",
    "            if metrics['silhouette'] > best_score:\n",
    "                best_score, best_gmm, best_n = metrics['silhouette'], gmm, n\n",
    "        return best_gmm, best_n, pd.DataFrame(results)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_top_features(original_features, scaled_features, feature_names, clusters):\n",
    "        unique_clusters = np.unique(clusters)\n",
    "        cluster_centers = []\n",
    "        for cluster in unique_clusters:\n",
    "            cluster_mask = clusters == cluster\n",
    "            cluster_mean = np.mean(scaled_features[cluster_mask], axis=0)\n",
    "            cluster_centers.append(cluster_mean)\n",
    "        cluster_centers = np.array(cluster_centers)\n",
    "        feature_importance = np.var(cluster_centers, axis=0)\n",
    "\n",
    "        # Perform ANOVA test for all features\n",
    "        pvals = []\n",
    "        for i in range(scaled_features.shape[1]):\n",
    "            groups = [scaled_features[clusters == c, i] for c in unique_clusters]\n",
    "            _, p = f_oneway(*groups)\n",
    "            pvals.append(p)\n",
    "        \n",
    "        # Apply FDR correction\n",
    "        reject, pvals_corrected, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
    "        \n",
    "        # Get all statistically significant features\n",
    "        sig_indices = np.where(reject)[0]\n",
    "        \n",
    "        # Get top 50 features for heatmap (sorted by importance)\n",
    "        if len(sig_indices) > 0:\n",
    "            top_sig = sig_indices[np.argsort(feature_importance[sig_indices])][-CONFIG['analysis']['top_features']:]\n",
    "            heatmap_indices = top_sig[::-1]\n",
    "        else:\n",
    "            heatmap_indices = []\n",
    "            print(\"Warning: No statistically significant features found at alpha=0.05\")\n",
    "\n",
    "        # Create summary for ALL statistically significant features\n",
    "        summary_data = []\n",
    "        if len(sig_indices) > 0:\n",
    "            # Sort all significant features by importance\n",
    "            all_sig_sorted = sig_indices[np.argsort(feature_importance[sig_indices])][::-1]\n",
    "            \n",
    "            for i in all_sig_sorted:\n",
    "                feature = feature_names[i]\n",
    "                \n",
    "                value_repr = []\n",
    "                original_value_repr = []\n",
    "                for cluster in unique_clusters:\n",
    "                    scaled_values = scaled_features[clusters == cluster, i]\n",
    "                    value_repr.append(f\"{np.median(scaled_values):.3f}({np.percentile(scaled_values, 25):.3f},{np.percentile(scaled_values, 75):.3f})\")\n",
    "                    original_values = original_features[clusters == cluster, i]\n",
    "                    original_value_repr.append(f\"{np.median(original_values):.3f}({np.percentile(original_values, 25):.3f},{np.percentile(original_values, 75):.3f})\")\n",
    "\n",
    "                summary_data.append({\n",
    "                    'Feature': feature,\n",
    "                    'Importance': feature_importance[i],\n",
    "                    'P_value': pvals[i],\n",
    "                    'P_value_corrected': pvals_corrected[i],\n",
    "                    **{f'Cluster{cluster}_Scaled': value_repr[cluster] for cluster in unique_clusters},\n",
    "                    **{f'Cluster{cluster}_Original': original_value_repr[cluster] for cluster in unique_clusters}\n",
    "                })\n",
    "\n",
    "        # Create heatmap data (only top 50 features)\n",
    "        if len(heatmap_indices) > 0:\n",
    "            df_heatmap = pd.DataFrame(scaled_features[:, heatmap_indices], \n",
    "                                    columns=[feature_names[i] for i in heatmap_indices])\n",
    "            df_heatmap['Cluster'] = clusters\n",
    "            df_heatmap = df_heatmap.sort_values('Cluster').drop('Cluster', axis=1)\n",
    "            heatmap_features = [feature_names[i] for i in heatmap_indices]\n",
    "        else:\n",
    "            df_heatmap = pd.DataFrame()\n",
    "            heatmap_features = []\n",
    "\n",
    "        return (pd.DataFrame(summary_data).sort_values('Importance', ascending=False),\n",
    "                df_heatmap,\n",
    "                heatmap_features)\n",
    "\n",
    "\n",
    "class Visualizer:\n",
    "    @staticmethod\n",
    "    def create_umap_plot(umap_features, clusters, best_silhouette, n_clusters):\n",
    "        fig, ax = plt.subplots(figsize=CONFIG['visualization']['figsize'])\n",
    "        scatter = ax.scatter(\n",
    "            umap_features[:, 0], umap_features[:, 1],\n",
    "            c=clusters, cmap=CONFIG['visualization']['cmap'], s=30, alpha=0.6\n",
    "        )\n",
    "        ax.set_title(f'UMAP Visualization of GMM Clusters (n={n_clusters})', fontsize=14)\n",
    "        ax.set_xlabel('UMAP 1', fontsize=12)\n",
    "        ax.set_ylabel('UMAP 2', fontsize=12)\n",
    "        cbar = plt.colorbar(scatter, ax=ax)\n",
    "        cbar.set_label('Cluster', fontsize=12)\n",
    "        ax.annotate(\n",
    "            f'Silhouette: {best_silhouette:.2f}',\n",
    "            xy=(0.05, 0.95), xycoords='axes fraction', fontsize=12,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    @staticmethod\n",
    "    def create_heatmap(top_features_heatmap, clusters):\n",
    "        if top_features_heatmap.empty:\n",
    "            return None\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=CONFIG['visualization']['heatmap_figsize'])\n",
    "        sns.heatmap(\n",
    "            top_features_heatmap.T,\n",
    "            cmap=CONFIG['visualization']['cmap'], ax=ax,\n",
    "            cbar_kws={'label': 'Scaled Value'}\n",
    "        )\n",
    "        \n",
    "        unique_clusters = np.unique(clusters)\n",
    "        cluster_counts = [np.sum(clusters == c) for c in unique_clusters]\n",
    "        cluster_boundaries = np.cumsum(cluster_counts)\n",
    "        \n",
    "        for i, (cluster, pos) in enumerate(zip(unique_clusters, cluster_boundaries)):\n",
    "            start = 0 if i == 0 else cluster_boundaries[i-1]\n",
    "            middle = (start + pos) / 2\n",
    "            ax.axvline(x=pos, color='black', linewidth=0.5)\n",
    "            ax.text(middle, -0.5, f'Cluster {cluster}', ha='center', va='center', fontsize=10)\n",
    "        \n",
    "        ax.set_title(f'Top {min(CONFIG[\"analysis\"][\"top_features\"], len(top_features_heatmap.columns))} Features Heatmap', fontsize=14)\n",
    "        ax.set_xlabel('Patient Index', fontsize=12)\n",
    "        ax.set_ylabel('Feature', fontsize=12)\n",
    "        ax.set_yticks(np.arange(len(top_features_heatmap.columns)) + 0.5)\n",
    "        ax.set_yticklabels(top_features_heatmap.columns, rotation=0, fontsize=8)\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "\n",
    "class ResultSaver:\n",
    "    @staticmethod\n",
    "    def save_results(cluster_results, metrics_df, feature_summary):\n",
    "        results_dir = CONFIG['output']['results_dir']\n",
    "        \n",
    "        # Save cluster assignments\n",
    "        cluster_results.to_csv(f'{results_dir}/cluster_assignments.csv', index=False)\n",
    "        \n",
    "        # Save clustering metrics\n",
    "        metrics_df.to_csv(f'{results_dir}/clustering_metrics.csv', index=False)\n",
    "        \n",
    "        # Save feature summary as radiomics_distinctive.csv\n",
    "        if not feature_summary.empty:\n",
    "            feature_summary.to_csv(f'{results_dir}/radiomics_distinctive.csv', index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    patient_ids, original_features, feature_names = DataProcessor.load_data()\n",
    "    \n",
    "    print(\"Selecting features...\")\n",
    "    features_selected, selected_feature_names = DataProcessor.select_features(original_features, feature_names)\n",
    "    \n",
    "    print(\"Scaling features...\")\n",
    "    features_scaled = DataProcessor.scale_features(features_selected)\n",
    "\n",
    "    print(\"Reducing dimensionality...\")\n",
    "    features_umap = DataProcessor.reduce_dimension(features_scaled)\n",
    "    \n",
    "    print(\"Optimizing GMM clustering...\")\n",
    "    best_gmm, best_n, metrics_df = ClusterAnalyzer.optimize_gmm(features_umap)\n",
    "    clusters = best_gmm.predict(features_umap)\n",
    "    best_silhouette = metrics_df.loc[metrics_df['n_clusters'] == best_n, 'silhouette'].values[0]\n",
    "\n",
    "    print(\"Analyzing top features...\")\n",
    "    top_features_summary, top_features_heatmap, top_feature_names = ClusterAnalyzer.get_top_features(\n",
    "        features_selected, features_scaled, selected_feature_names, clusters\n",
    "    )\n",
    "\n",
    "    print(\"Creating 2D visualization...\")\n",
    "    umap_2d = umap.UMAP(\n",
    "        n_components=CONFIG['dimension_reduction']['final_components'],\n",
    "        n_neighbors=CONFIG['dimension_reduction']['n_neighbors'],\n",
    "        min_dist=CONFIG['dimension_reduction']['min_dist'],\n",
    "        metric=CONFIG['dimension_reduction']['metric'],\n",
    "        random_state=CONFIG['dimension_reduction']['random_state']\n",
    "    ).fit_transform(features_scaled)\n",
    "\n",
    "    print(\"Generating visualizations...\")\n",
    "    # Create and save UMAP plot\n",
    "    umap_fig = Visualizer.create_umap_plot(umap_2d, clusters, best_silhouette, best_n)\n",
    "    umap_fig.savefig(f\"{CONFIG['output']['figures_dir']}/umap_clusters.png\", \n",
    "                     dpi=CONFIG['visualization']['dpi'], bbox_inches='tight')\n",
    "    plt.close(umap_fig)\n",
    "\n",
    "    # Create and save heatmap\n",
    "    heatmap_fig = Visualizer.create_heatmap(top_features_heatmap, clusters)\n",
    "    if heatmap_fig is not None:\n",
    "        heatmap_fig.savefig(f\"{CONFIG['output']['figures_dir']}/TOP50_heatmap.png\", \n",
    "                            dpi=CONFIG['visualization']['dpi'], bbox_inches='tight')\n",
    "        plt.close(heatmap_fig)\n",
    "\n",
    "    print(\"Saving results...\")\n",
    "    ResultSaver.save_results(\n",
    "        pd.DataFrame({'PatientID': patient_ids, 'Cluster': clusters}),\n",
    "        metrics_df,\n",
    "        top_features_summary\n",
    "    )\n",
    "\n",
    "    print(f\"Optimal number of clusters: {best_n}\")\n",
    "    print(f\"Best silhouette score: {best_silhouette:.2f}\")\n",
    "    if not top_features_summary.empty:\n",
    "        print(f\"Total statistically significant features: {len(top_features_summary)}\")\n",
    "        print(f\"Most important feature: {top_features_summary.iloc[0]['Feature']} \"\n",
    "              f\"(importance={top_features_summary.iloc[0]['Importance']:.3f})\")\n",
    "    else:\n",
    "        print(\"No statistically significant features found\")\n",
    "    print(f\"Visualizations saved to {CONFIG['output']['figures_dir']}\")\n",
    "    print(f\"Results saved to {CONFIG['output']['results_dir']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radiomics",
   "language": "python",
   "name": "rad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
