{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446feba3-9d45-4bd4-a42f-210c34c0815b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MACHINE LEARNING ANALYSIS FOR RECURRENCE PREDICTION\n",
      "============================================================\n",
      "Loading data...\n",
      "Clinical data shape: (142, 15)\n",
      "Pathological data shape: (142, 7)\n",
      "Recurrence data shape: (142, 2)\n",
      "\n",
      "Extracting radiomics features...\n",
      "Selected 50 distinctive radiomics features\n",
      "Radiomics features shape: (205, 50)\n",
      "Common IDs across all datasets: 142\n",
      "All features combined shape: (142, 72)\n",
      "Target distribution: 1    86\n",
      "0    56\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature group indices:\n",
      "Clinical features: 0 to 14 (15 features)\n",
      "Pathological features: 15 to 21 (7 features)\n",
      "Radiomics features: 22 to 71 (50 features)\n",
      "Categorical features: 15\n",
      "Numerical features: 57\n",
      "Handling missing values...\n",
      "No missing values found.\n",
      "Applying adaptive PCA to radiomics features...\n",
      "Original radiomics features shape: (142, 50)\n",
      "PCA explained variance ratio: 0.952\n",
      "Number of PCA components: 9\n",
      "PCA features shape: (142, 9)\n",
      "Saving PCA component formulas...\n",
      "PCA component formulas saved to ./results/pca_components_formulas.txt\n",
      "\n",
      "Feature combinations with PCA:\n",
      "Clinical: 15 features\n",
      "Clinical+Pathological: 22 features\n",
      "Clinical+Radiomics: 24 features\n",
      "Clinical+Pathological+Radiomics: 31 features\n",
      "Scaling numerical features...\n",
      "NESTED CROSS-VALIDATION WITH FEATURE STABILITY ANALYSIS\n",
      "\n",
      "Analyzing feature combination: Clinical\n",
      "Training LogisticRegression...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 2 stable features\n",
      "AUC: 0.724 (0.711-0.736)\n",
      "Training SVM...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 2 stable features\n",
      "AUC: 0.717 (0.704-0.730)\n",
      "Training DecisionTree...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 2 stable features\n",
      "AUC: 0.699 (0.687-0.712)\n",
      "Training RandomForest...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 2 stable features\n",
      "AUC: 0.715 (0.702-0.728)\n",
      "Training KNN...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 2 stable features\n",
      "AUC: 0.702 (0.688-0.716)\n",
      "Training NaiveBayes...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 2 stable features\n",
      "AUC: 0.685 (0.672-0.699)\n",
      "Training XGBoost...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 2 stable features\n",
      "AUC: 0.722 (0.710-0.735)\n",
      "Best model: LogisticRegression (AUC: 0.724)\n",
      "\n",
      "Analyzing feature combination: Clinical+Pathological\n",
      "Training LogisticRegression...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 8 stable features\n",
      "AUC: 0.888 (0.878-0.897)\n",
      "Training SVM...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 8 stable features\n",
      "AUC: 0.873 (0.850-0.896)\n",
      "Training DecisionTree...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 8 stable features\n",
      "AUC: 0.826 (0.812-0.840)\n",
      "Training RandomForest...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 8 stable features\n",
      "AUC: 0.890 (0.879-0.901)\n",
      "Training KNN...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 8 stable features\n",
      "AUC: 0.854 (0.842-0.866)\n",
      "Training NaiveBayes...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 8 stable features\n",
      "AUC: 0.874 (0.863-0.884)\n",
      "Training XGBoost...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 8 stable features\n",
      "AUC: 0.878 (0.868-0.889)\n",
      "Best model: RandomForest (AUC: 0.890)\n",
      "\n",
      "Analyzing feature combination: Clinical+Radiomics\n",
      "Training LogisticRegression...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 4 stable features\n",
      "AUC: 0.836 (0.826-0.847)\n",
      "Training SVM...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 4 stable features\n",
      "AUC: 0.832 (0.819-0.845)\n",
      "Training DecisionTree...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 4 stable features\n",
      "AUC: 0.782 (0.766-0.798)\n",
      "Training RandomForest...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 4 stable features\n",
      "AUC: 0.815 (0.805-0.826)\n",
      "Training KNN...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 4 stable features\n",
      "AUC: 0.784 (0.770-0.799)\n",
      "Training NaiveBayes...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 4 stable features\n",
      "AUC: 0.823 (0.811-0.835)\n",
      "Training XGBoost...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 4 stable features\n",
      "AUC: 0.809 (0.797-0.821)\n",
      "Best model: LogisticRegression (AUC: 0.836)\n",
      "\n",
      "Analyzing feature combination: Clinical+Pathological+Radiomics\n",
      "Training LogisticRegression...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 10 stable features\n",
      "AUC: 0.909 (0.900-0.918)\n",
      "Training SVM...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 10 stable features\n",
      "AUC: 0.907 (0.898-0.916)\n",
      "Training DecisionTree...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 10 stable features\n",
      "AUC: 0.777 (0.762-0.791)\n",
      "Training RandomForest...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 10 stable features\n",
      "AUC: 0.880 (0.869-0.891)\n",
      "Training KNN...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 10 stable features\n",
      "AUC: 0.841 (0.828-0.855)\n",
      "Training NaiveBayes...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 10 stable features\n",
      "AUC: 0.884 (0.874-0.894)\n",
      "Training XGBoost...\n",
      "Running unified nested CV: 20 repeats × 5 folds\n",
      "Found 10 stable features\n",
      "AUC: 0.870 (0.859-0.881)\n",
      "Best model: LogisticRegression (AUC: 0.909)\n",
      "\n",
      "PERFORMING STATISTICAL TESTS\n",
      "============================================================\n",
      "\n",
      "Pairwise AUC Comparisons (p-values):\n",
      "                              Clinical       Clinical+PathologicalClinical+RadiomicsClinical+Pathological+Radiomics\n",
      "Clinical                      --             0.0000*             0.0000*             0.0000*             \n",
      "Clinical+Pathological         0.0000*             --             0.0000*             0.0000*             \n",
      "Clinical+Radiomics            0.0000*             0.0000*             --             0.0000*             \n",
      "Clinical+Pathological+Radiomics0.0000*             0.0000*             0.0000*             --             \n",
      "\n",
      "Best performing feature combination: Clinical+Pathological+Radiomics\n",
      "Best AUC: 0.909\n",
      "\n",
      "Comparison with best model:\n",
      "Clinical+Pathological+Radiomics vs Clinical: p = 0.0000 (SIGNIFICANT)\n",
      "Clinical+Pathological+Radiomics vs Clinical+Pathological: p = 0.0000 (SIGNIFICANT)\n",
      "Clinical+Pathological+Radiomics vs Clinical+Radiomics: p = 0.0000 (SIGNIFICANT)\n",
      "Processing stable features...\n",
      "Processed 15 features for Clinical\n",
      "Found 2 stable features for Clinical\n",
      "Processed 22 features for Clinical+Pathological\n",
      "Found 8 stable features for Clinical+Pathological\n",
      "Processed 24 features for Clinical+Radiomics\n",
      "Found 4 stable features for Clinical+Radiomics\n",
      "Processed 31 features for Clinical+Pathological+Radiomics\n",
      "Found 10 stable features for Clinical+Pathological+Radiomics\n",
      "\n",
      "Creating visualizations...\n",
      "Plotting performance distribution...\n",
      "Plotting feature stability...\n",
      "Creating SHAP plots...\n",
      "Training final model for SHAP explanation: Clinical\n",
      "SHAP plots created for Clinical\n",
      "Training final model for SHAP explanation: Clinical+Pathological\n",
      "SHAP plots created for Clinical+Pathological\n",
      "Training final model for SHAP explanation: Clinical+Radiomics\n",
      "SHAP plots created for Clinical+Radiomics\n",
      "Training final model for SHAP explanation: Clinical+Pathological+Radiomics\n",
      "SHAP plots created for Clinical+Pathological+Radiomics\n",
      "\n",
      "DETAILED RESULTS SUMMARY\n",
      "====================================================================================================\n",
      "Feature Combination       Algorithm            Stable Features AUC (95% CI)         Sensitivity (95% CI)      Specificity (95% CI)     \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clinical                  LogisticRegression   2               0.724 (0.711-0.736)  0.966 (0.950-0.982)       0.123 (0.096-0.150)      \n",
      "Clinical+Pathological     RandomForest         8               0.890 (0.879-0.901)  0.823 (0.805-0.841)       0.809 (0.787-0.831)      \n",
      "Clinical+Radiomics        LogisticRegression   4               0.836 (0.826-0.847)  0.755 (0.735-0.776)       0.746 (0.716-0.776)      \n",
      "Clinical+Pathological+Radiomics LogisticRegression   10              0.909 (0.900-0.918)  0.831 (0.811-0.850)       0.858 (0.838-0.879)       *\n",
      "\n",
      "* Best performing feature combination\n",
      "Saving results to CSV files...\n",
      "Saved results to ./results/nested_cv_results.csv\n",
      "Saved statistical comparison to ./results/nested_cv_statistical_comparison.csv\n",
      "Saving figures as PNG files...\n",
      "Saved performance distribution as ./figures/performance_distribution.png\n",
      "Saved statistical comparison as ./figures/statistical_comparison.png\n",
      "Saved Feature_Stability_Clinical as ./figures/Feature_Stability_Clinical.png\n",
      "Saved Feature_Stability_Clinical+Pathological as ./figures/Feature_Stability_Clinical_Pathological.png\n",
      "Saved Feature_Stability_Clinical+Radiomics as ./figures/Feature_Stability_Clinical_Radiomics.png\n",
      "Saved Feature_Stability_Clinical+Pathological+Radiomics as ./figures/Feature_Stability_Clinical_Pathological_Radiomics.png\n",
      "Saved SHAP_Bar_Clinical as ./figures/SHAP_Bar_Clinical.png\n",
      "Saved SHAP_Beeswarm_Clinical as ./figures/SHAP_Beeswarm_Clinical.png\n",
      "Saved SHAP_Bar_Clinical+Pathological as ./figures/SHAP_Bar_Clinical_Pathological.png\n",
      "Saved SHAP_Beeswarm_Clinical+Pathological as ./figures/SHAP_Beeswarm_Clinical_Pathological.png\n",
      "Saved SHAP_Bar_Clinical+Radiomics as ./figures/SHAP_Bar_Clinical_Radiomics.png\n",
      "Saved SHAP_Beeswarm_Clinical+Radiomics as ./figures/SHAP_Beeswarm_Clinical_Radiomics.png\n",
      "Saved SHAP_Bar_Clinical+Pathological+Radiomics as ./figures/SHAP_Bar_Clinical_Pathological_Radiomics.png\n",
      "Saved SHAP_Beeswarm_Clinical+Pathological+Radiomics as ./figures/SHAP_Beeswarm_Clinical_Pathological_Radiomics.png\n",
      "All figures saved successfully to ./figures/ folder!\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "Results saved to: ./results/\n",
      "Figures saved to: ./figures/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFdr, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (roc_auc_score, accuracy_score, f1_score, \n",
    "                            recall_score, precision_score, confusion_matrix, \n",
    "                            roc_curve, auc)\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import t\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "import os\n",
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "# 设置随机种子\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# 交叉验证参数\n",
    "N_OUTER_REPEATS = 20\n",
    "N_OUTER_SPLITS = 5\n",
    "N_INNER_SPLITS = 5\n",
    "FDR_ALPHA = 0.05\n",
    "PCA_VARIANCE_THRESHOLD = 0.95\n",
    "FEATURE_FREQ_THRESHOLD = 0.6\n",
    "SHAP_NSAMPLES = 100\n",
    "SHAP_BACKGROUND_SAMPLES = 50\n",
    "\n",
    "# 模型参数\n",
    "LOGISTIC_REG_PARAMS = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "SVM_PARAMS = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "DECISION_TREE_PARAMS = {\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "    'min_samples_leaf': [3, 5, 7],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "RANDOM_FOREST_PARAMS = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [3, 5],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "}\n",
    "\n",
    "KNN_PARAMS = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "NAIVE_BAYES_PARAMS = {\n",
    "    'var_smoothing': [1e-11, 1e-9, 1e-7, 1e-5, 1e-3]\n",
    "}\n",
    "\n",
    "XGBOOST_PARAMS = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [0, 0.1, 1],\n",
    "    'scale_pos_weight': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "RANDOM_SEARCH_MODELS = {\n",
    "    'DecisionTree': True,\n",
    "    'RandomForest': True, \n",
    "    'XGBoost': True\n",
    "}\n",
    "\n",
    "RANDOM_SEARCH_ITER = 50\n",
    "\n",
    "def save_pca_formulas(pca_model, feature_names, explained_variance_ratios, filename='./results/pca_components_formulas.txt'):\n",
    "    \"\"\"Save PCA component formulas to a text file\"\"\"\n",
    "    print(\"Saving PCA component formulas...\")\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"PCA Component Formulas\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        for i in range(pca_model.n_components_):\n",
    "            f.write(f\"PC{i+1} (Variance Explained: {explained_variance_ratios[i]:.4f}):\\n\")\n",
    "            f.write(\"Formula: PC{} = \".format(i+1))\n",
    "            \n",
    "            # Get the component weights and sort by absolute value\n",
    "            component_weights = pca_model.components_[i]\n",
    "            feature_weights = list(zip(feature_names, component_weights))\n",
    "            feature_weights.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "            \n",
    "            # Write the top contributing features\n",
    "            terms = []\n",
    "            for feature, weight in feature_weights[:10]:  # Top 10 features per component\n",
    "                if abs(weight) > 0.01:  # Only include meaningful contributions\n",
    "                    terms.append(f\"{weight:.4f} × {feature}\")\n",
    "            \n",
    "            f.write(\" + \".join(terms))\n",
    "            f.write(\"\\n\\n\")\n",
    "            \n",
    "            # Also write the detailed feature contributions\n",
    "            f.write(\"Top 10 feature contributions:\\n\")\n",
    "            for j, (feature, weight) in enumerate(feature_weights[:10]):\n",
    "                f.write(f\"  {j+1:2d}. {feature}: {weight:.4f}\\n\")\n",
    "            f.write(\"\\n\" + \"-\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"PCA component formulas saved to {filename}\")\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load clinical, pathological, recurrence data and extract radiomics features\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load clinical data\n",
    "    clinical_df = pd.read_csv('./data/clinical.csv', index_col=0)\n",
    "    print(f\"Clinical data shape: {clinical_df.shape}\")\n",
    "    \n",
    "    # Load pathological data\n",
    "    patho_df = pd.read_csv('./data/pathological.csv', index_col=0)\n",
    "    print(f\"Pathological data shape: {patho_df.shape}\")\n",
    "    \n",
    "    # Load recurrence data\n",
    "    recurrence_df = pd.read_csv('./data/recurrence.csv', index_col=0)\n",
    "    print(f\"Recurrence data shape: {recurrence_df.shape}\")\n",
    "    \n",
    "    # Extract radiomics features\n",
    "    print(\"\\nExtracting radiomics features...\")\n",
    "    # Load distinctive radiomics features\n",
    "    distinctive_df = pd.read_csv('./results/radiomics_distinctive.csv')\n",
    "    # Get top 50 features (already sorted by importance)\n",
    "    top_50_features = distinctive_df['Feature'].head(50).tolist()\n",
    "    print(f\"Selected {len(top_50_features)} distinctive radiomics features\")\n",
    "    \n",
    "    # Load all radiomics features\n",
    "    radiomics_df = pd.read_csv('./results/radiomics.csv', index_col=0)\n",
    "    # Extract only the top 50 features\n",
    "    radiomics_features = radiomics_df[top_50_features]\n",
    "    print(f\"Radiomics features shape: {radiomics_features.shape}\")\n",
    "    \n",
    "    # Merge all data\n",
    "    # First, get common IDs\n",
    "    common_ids = list(set(clinical_df.index) & set(patho_df.index) & \n",
    "                      set(recurrence_df.index) & set(radiomics_features.index))\n",
    "    print(f\"Common IDs across all datasets: {len(common_ids)}\")\n",
    "    \n",
    "    # Align all dataframes\n",
    "    clinical_aligned = clinical_df.loc[common_ids]\n",
    "    patho_aligned = patho_df.loc[common_ids]\n",
    "    recurrence_aligned = recurrence_df.loc[common_ids]\n",
    "    radiomics_aligned = radiomics_features.loc[common_ids]\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = pd.concat([clinical_aligned, patho_aligned, radiomics_aligned], axis=1)\n",
    "    print(f\"All features combined shape: {all_features.shape}\")\n",
    "    \n",
    "    # Get target (assume first column is recurrence)\n",
    "    y = recurrence_aligned.iloc[:, 0].values\n",
    "    print(f\"Target distribution: {pd.Series(y).value_counts()}\")\n",
    "    \n",
    "    # Define feature groups (for reference)\n",
    "    clinical_end = clinical_aligned.shape[1]\n",
    "    patho_end = clinical_end + patho_aligned.shape[1]\n",
    "    radio_start = patho_end\n",
    "    radio_end = all_features.shape[1]\n",
    "    \n",
    "    print(f\"\\nFeature group indices:\")\n",
    "    print(f\"Clinical features: 0 to {clinical_end-1} ({clinical_end} features)\")\n",
    "    print(f\"Pathological features: {clinical_end} to {patho_end-1} ({patho_end-clinical_end} features)\")\n",
    "    print(f\"Radiomics features: {patho_end} to {radio_end-1} ({radio_end-patho_end} features)\")\n",
    "    \n",
    "    return all_features, y, clinical_end, patho_end, radio_end\n",
    "\n",
    "def identify_feature_types(all_features):\n",
    "    \"\"\"Identify categorical and numerical features\"\"\"\n",
    "    \n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "    \n",
    "    for col in all_features.columns:\n",
    "        unique_vals = all_features[col].nunique()\n",
    "        if unique_vals < 6:\n",
    "            categorical_features.append(col)\n",
    "        else:\n",
    "            numerical_features.append(col)\n",
    "    \n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"Numerical features: {len(numerical_features)}\")\n",
    "    \n",
    "    return categorical_features, numerical_features\n",
    "\n",
    "def handle_missing_values(all_features, categorical_features, numerical_features):\n",
    "    \"\"\"Handle missing values in the dataset\"\"\"\n",
    "    print(\"Handling missing values...\")\n",
    "    \n",
    "    missing_values = all_features.isnull().sum()\n",
    "    features_with_missing = missing_values[missing_values > 0]\n",
    "    \n",
    "    if len(features_with_missing) > 0:\n",
    "        print(f\"Features with missing values: {features_with_missing}\")\n",
    "        \n",
    "        for col in all_features.columns:\n",
    "            if col in categorical_features:\n",
    "                mode_val = all_features[col].mode()\n",
    "                if len(mode_val) > 0:\n",
    "                    all_features[col].fillna(mode_val[0], inplace=True)\n",
    "                else:\n",
    "                    all_features[col].fillna(0, inplace=True)\n",
    "            else:\n",
    "                all_features[col].fillna(all_features[col].median(), inplace=True)\n",
    "        \n",
    "        print(\"Missing values filled.\")\n",
    "    else:\n",
    "        print(\"No missing values found.\")\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "def adaptive_pca(radio_features, variance_threshold=PCA_VARIANCE_THRESHOLD):\n",
    "    \"\"\"Apply adaptive PCA to radiomics features\"\"\"\n",
    "    print(\"Applying adaptive PCA to radiomics features...\")\n",
    "    print(f\"Original radiomics features shape: {radio_features.shape}\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    radio_scaled = scaler.fit_transform(radio_features)\n",
    "    \n",
    "    pca = PCA(n_components=variance_threshold, random_state=RANDOM_STATE)\n",
    "    radio_pca = pca.fit_transform(radio_scaled)\n",
    "    \n",
    "    n_components = radio_pca.shape[1]\n",
    "    explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    pca_feature_names = [f'PC{i+1}' for i in range(n_components)]\n",
    "    radio_pca_df = pd.DataFrame(radio_pca, columns=pca_feature_names, index=radio_features.index)\n",
    "    \n",
    "    print(f\"PCA explained variance ratio: {explained_variance:.3f}\")\n",
    "    print(f\"Number of PCA components: {n_components}\")\n",
    "    print(f\"PCA features shape: {radio_pca_df.shape}\")\n",
    "    \n",
    "    return radio_pca_df, pca, scaler\n",
    "\n",
    "def prepare_feature_combinations_with_pca(all_features, clinical_end, patho_end, radio_end):\n",
    "    \"\"\"Prepare different feature combinations with PCA applied to radiomics\"\"\"\n",
    "    \n",
    "    # Extract feature groups\n",
    "    clinical_features = all_features.iloc[:, :clinical_end]\n",
    "    patho_features = all_features.iloc[:, clinical_end:patho_end]\n",
    "    radio_features = all_features.iloc[:, patho_end:radio_end]\n",
    "    \n",
    "    # Apply PCA to radiomics features\n",
    "    radio_pca_df, pca_model, pca_scaler = adaptive_pca(radio_features)\n",
    "    \n",
    "    # Save PCA component formulas\n",
    "    save_pca_formulas(pca_model, radio_features.columns.tolist(), \n",
    "                      pca_model.explained_variance_ratio_)\n",
    "    \n",
    "    # Create feature combinations\n",
    "    feature_combinations = {\n",
    "        'Clinical': clinical_features.copy(),\n",
    "        'Clinical+Pathological': pd.concat([clinical_features, patho_features], axis=1).copy(),\n",
    "        'Clinical+Radiomics': pd.concat([clinical_features, radio_pca_df], axis=1).copy(),\n",
    "        'Clinical+Pathological+Radiomics': pd.concat([clinical_features, patho_features, radio_pca_df], axis=1).copy()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nFeature combinations with PCA:\")\n",
    "    for name, features in feature_combinations.items():\n",
    "        print(f\"{name}: {features.shape[1]} features\")\n",
    "    \n",
    "    return feature_combinations, pca_model, pca_scaler, clinical_features, patho_features, radio_features\n",
    "\n",
    "def scale_features(feature_combinations, numerical_features):\n",
    "    \"\"\"Scale numerical features\"\"\"\n",
    "    print(\"Scaling numerical features...\")\n",
    "    \n",
    "    scaled_combinations = {}\n",
    "    for name, features in feature_combinations.items():\n",
    "        scaled_features = features.copy()\n",
    "        if len(numerical_features) > 0:\n",
    "            numerical_cols = [col for col in numerical_features if col in scaled_features.columns]\n",
    "            if len(numerical_cols) > 0:\n",
    "                scaler = StandardScaler()\n",
    "                scaled_features[numerical_cols] = scaler.fit_transform(scaled_features[numerical_cols])\n",
    "        scaled_combinations[name] = scaled_features\n",
    "    \n",
    "    return scaled_combinations\n",
    "\n",
    "def get_model_configurations():\n",
    "    \"\"\"Get model configurations for training\"\"\"\n",
    "    models = {\n",
    "        'LogisticRegression': {\n",
    "            'model': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "            'params': LOGISTIC_REG_PARAMS,\n",
    "            'use_random_search': False\n",
    "        },\n",
    "        'SVM': {\n",
    "            'model': SVC(random_state=RANDOM_STATE, probability=True),\n",
    "            'params': SVM_PARAMS,\n",
    "            'use_random_search': False\n",
    "        },\n",
    "        'DecisionTree': {\n",
    "            'model': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "            'params': DECISION_TREE_PARAMS,\n",
    "            'use_random_search': RANDOM_SEARCH_MODELS['DecisionTree']\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "            'params': RANDOM_FOREST_PARAMS,\n",
    "            'use_random_search': RANDOM_SEARCH_MODELS['RandomForest']\n",
    "        },\n",
    "        'KNN': {\n",
    "            'model': KNeighborsClassifier(),\n",
    "            'params': KNN_PARAMS,\n",
    "            'use_random_search': False\n",
    "        },\n",
    "        'NaiveBayes': {\n",
    "            'model': GaussianNB(),\n",
    "            'params': NAIVE_BAYES_PARAMS,\n",
    "            'use_random_search': False\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'model': XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'),\n",
    "            'params': XGBOOST_PARAMS,\n",
    "            'use_random_search': RANDOM_SEARCH_MODELS['XGBoost']\n",
    "        }\n",
    "    }\n",
    "    return models\n",
    "\n",
    "def run_single_cv_iteration(X_train, X_test, y_train, y_test, feature_names, model_config, apply_feature_selection=True):\n",
    "    \"\"\"Run single cross-validation iteration\"\"\"\n",
    "    if apply_feature_selection:\n",
    "        selector = SelectFdr(score_func=f_classif, alpha=FDR_ALPHA)\n",
    "        try:\n",
    "            X_train_processed = selector.fit_transform(X_train, y_train)\n",
    "            selected_indices = selector.get_support(indices=True)\n",
    "            selected_features = [feature_names[i] for i in selected_indices]\n",
    "            X_test_processed = selector.transform(X_test)\n",
    "        except:\n",
    "            selected_features = feature_names\n",
    "            X_train_processed = X_train\n",
    "            X_test_processed = X_test\n",
    "    else:\n",
    "        selected_features = feature_names\n",
    "        X_train_processed = X_train\n",
    "        X_test_processed = X_test\n",
    "    \n",
    "    inner_cv = StratifiedKFold(n_splits=N_INNER_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    if len(model_config['params']) > 0:\n",
    "        if model_config['use_random_search']:\n",
    "            search = RandomizedSearchCV(\n",
    "                model_config['model'], \n",
    "                model_config['params'], \n",
    "                cv=inner_cv, \n",
    "                scoring='roc_auc', \n",
    "                n_jobs=-1,\n",
    "                n_iter=RANDOM_SEARCH_ITER,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "        else:\n",
    "            search = GridSearchCV(\n",
    "                model_config['model'], \n",
    "                model_config['params'], \n",
    "                cv=inner_cv, \n",
    "                scoring='roc_auc', \n",
    "                n_jobs=-1\n",
    "            )\n",
    "        \n",
    "        search.fit(X_train_processed, y_train)\n",
    "        best_model = search.best_estimator_\n",
    "    else:\n",
    "        best_model = copy.deepcopy(model_config['model'])\n",
    "        best_model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    y_pred_proba = best_model.predict_proba(X_test_processed)[:, 1]\n",
    "    y_pred = best_model.predict(X_test_processed)\n",
    "    \n",
    "    metrics = {\n",
    "        'auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'acc': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'sens': recall_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    metrics['spec'] = tn / (tn + fp)\n",
    "    \n",
    "    return metrics, selected_features, best_model, y_pred_proba, y_test\n",
    "\n",
    "def unified_nested_cv(X, y, feature_names, model_config):\n",
    "    \"\"\"Perform unified nested cross-validation\"\"\"\n",
    "    outer_cv = RepeatedStratifiedKFold(\n",
    "        n_splits=N_OUTER_SPLITS, \n",
    "        n_repeats=N_OUTER_REPEATS, \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    all_metrics = {\n",
    "        'auc': [], 'acc': [], 'f1': [], 'sens': [], 'spec': []\n",
    "    }\n",
    "    all_selected_features = []\n",
    "    all_y_true = []\n",
    "    all_y_pred_proba = []\n",
    "    \n",
    "    print(f\"Running unified nested CV: {N_OUTER_REPEATS} repeats × {N_OUTER_SPLITS} folds\")\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        metrics, selected_features, _, y_pred_proba, y_test_vals = run_single_cv_iteration(\n",
    "            X_train, X_test, y_train, y_test, feature_names, model_config, apply_feature_selection=True\n",
    "        )\n",
    "        \n",
    "        all_selected_features.extend(selected_features)\n",
    "        all_y_true.extend(y_test_vals)\n",
    "        all_y_pred_proba.extend(y_pred_proba)\n",
    "        \n",
    "        for key in all_metrics:\n",
    "            all_metrics[key].append(metrics[key])\n",
    "    \n",
    "    stable_features, feature_frequencies = find_stable_features(all_selected_features, feature_names)\n",
    "    \n",
    "    print(f\"Found {len(stable_features)} stable features\")\n",
    "    \n",
    "    stable_metrics = {\n",
    "        'auc': [], 'acc': [], 'f1': [], 'sens': [], 'spec': []\n",
    "    }\n",
    "    stable_y_true = []\n",
    "    stable_y_pred_proba = []\n",
    "    \n",
    "    stable_indices = [i for i, feature in enumerate(feature_names) if feature in stable_features]\n",
    "    X_stable = X[:, stable_indices]\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(outer_cv.split(X_stable, y)):\n",
    "        X_train, X_test = X_stable[train_idx], X_stable[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        metrics, _, _, y_pred_proba, y_test_vals = run_single_cv_iteration(\n",
    "            X_train, X_test, y_train, y_test, stable_features, model_config, apply_feature_selection=False\n",
    "        )\n",
    "        \n",
    "        stable_y_true.extend(y_test_vals)\n",
    "        stable_y_pred_proba.extend(y_pred_proba)\n",
    "        \n",
    "        for key in stable_metrics:\n",
    "            stable_metrics[key].append(metrics[key])\n",
    "    \n",
    "    return all_metrics, stable_metrics, stable_features, feature_frequencies, stable_y_true, stable_y_pred_proba\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = np.std(data, ddof=1) / np.sqrt(n)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean, mean - h, mean + h\n",
    "\n",
    "def find_stable_features(selected_features_list, feature_names, threshold=FEATURE_FREQ_THRESHOLD):\n",
    "    \"\"\"Find stable features based on selection frequency\"\"\"\n",
    "    feature_counts = Counter(selected_features_list)\n",
    "    total_iterations = N_OUTER_REPEATS * N_OUTER_SPLITS\n",
    "    \n",
    "    stable_features = []\n",
    "    feature_frequencies = {}\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        freq = feature_counts.get(feature, 0) / total_iterations\n",
    "        feature_frequencies[feature] = freq\n",
    "        if freq >= threshold:\n",
    "            stable_features.append(feature)\n",
    "    \n",
    "    return stable_features, feature_frequencies\n",
    "\n",
    "def save_stable_features_to_csv(feature_stability_results):\n",
    "    \"\"\"Process stable features data without saving CSV\"\"\"\n",
    "    print(f\"Processing stable features...\")\n",
    "    \n",
    "    all_stable_features_data = {}\n",
    "    \n",
    "    for feature_name, stability_info in feature_stability_results.items():\n",
    "        stable_features = stability_info['stable_features']\n",
    "        feature_frequencies = stability_info['feature_frequencies']\n",
    "        \n",
    "        freq_df = pd.DataFrame({\n",
    "            'Feature': list(feature_frequencies.keys()),\n",
    "            'Frequency': list(feature_frequencies.values())\n",
    "        }).sort_values('Frequency', ascending=False)\n",
    "        \n",
    "        freq_df['Stable'] = freq_df['Frequency'] >= FEATURE_FREQ_THRESHOLD\n",
    "        \n",
    "        all_stable_features_data[feature_name] = freq_df\n",
    "        print(f\"Processed {len(freq_df)} features for {feature_name}\")\n",
    "        \n",
    "        stable_only_df = freq_df[freq_df['Stable']]\n",
    "        print(f\"Found {len(stable_only_df)} stable features for {feature_name}\")\n",
    "    \n",
    "    return all_stable_features_data\n",
    "\n",
    "def perform_statistical_tests(results):\n",
    "    \"\"\"Perform statistical tests to compare best models\"\"\"\n",
    "    print(\"\\nPERFORMING STATISTICAL TESTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Extract AUC scores for statistical comparison\n",
    "    auc_data = {}\n",
    "    for feature_name, result in results.items():\n",
    "        auc_data[feature_name] = result['metrics_fixed']['auc']\n",
    "    \n",
    "    # Perform paired t-tests between all combinations\n",
    "    feature_names = list(auc_data.keys())\n",
    "    p_values = np.zeros((len(feature_names), len(feature_names)))\n",
    "    \n",
    "    print(\"\\nPairwise AUC Comparisons (p-values):\")\n",
    "    print(f\"{'':<30}\", end=\"\")\n",
    "    for name in feature_names:\n",
    "        print(f\"{name:<15}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i, name1 in enumerate(feature_names):\n",
    "        print(f\"{name1:<30}\", end=\"\")\n",
    "        for j, name2 in enumerate(feature_names):\n",
    "            if i == j:\n",
    "                p_values[i, j] = 1.0\n",
    "                print(f\"{'--':<15}\", end=\"\")\n",
    "            else:\n",
    "                t_stat, p_val = stats.ttest_rel(auc_data[name1], auc_data[name2])\n",
    "                p_values[i, j] = p_val\n",
    "                print(f\"{p_val:.4f}{'*' if p_val < 0.05 else '':<14}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Find the best performing feature combination\n",
    "    best_feature = max(results.keys(), key=lambda x: results[x]['auc_mean'])\n",
    "    print(f\"\\nBest performing feature combination: {best_feature}\")\n",
    "    print(f\"Best AUC: {results[best_feature]['auc_mean']:.3f}\")\n",
    "    \n",
    "    # Compare best with others\n",
    "    print(\"\\nComparison with best model:\")\n",
    "    for feature_name in feature_names:\n",
    "        if feature_name != best_feature:\n",
    "            t_stat, p_val = stats.ttest_rel(auc_data[best_feature], auc_data[feature_name])\n",
    "            significance = \"SIGNIFICANT\" if p_val < 0.05 else \"not significant\"\n",
    "            print(f\"{best_feature} vs {feature_name}: p = {p_val:.4f} ({significance})\")\n",
    "    \n",
    "    return p_values, best_feature\n",
    "\n",
    "def train_and_evaluate_with_nested_cv(feature_combinations, y):\n",
    "    \"\"\"Train and evaluate models with nested cross-validation\"\"\"\n",
    "    print(\"NESTED CROSS-VALIDATION WITH FEATURE STABILITY ANALYSIS\")\n",
    "    \n",
    "    results = {}\n",
    "    feature_stability_results = {}\n",
    "    prediction_data = {}\n",
    "    \n",
    "    for feature_name, features in feature_combinations.items():\n",
    "        print(f\"\\nAnalyzing feature combination: {feature_name}\")\n",
    "        \n",
    "        X = features.values\n",
    "        feature_names = features.columns.tolist()\n",
    "        \n",
    "        models_config = get_model_configurations()\n",
    "        feature_results = {}\n",
    "        \n",
    "        for model_name, config in models_config.items():\n",
    "            print(f\"Training {model_name}...\")\n",
    "            \n",
    "            metrics_with_fs, metrics_fixed, stable_features, feature_frequencies, y_true, y_pred_proba = unified_nested_cv(\n",
    "                X, y, feature_names, config\n",
    "            )\n",
    "            \n",
    "            auc_mean, auc_ci_lower, auc_ci_upper = calculate_confidence_interval(metrics_fixed['auc'])\n",
    "            sens_mean, sens_ci_lower, sens_ci_upper = calculate_confidence_interval(metrics_fixed['sens'])\n",
    "            spec_mean, spec_ci_lower, spec_ci_upper = calculate_confidence_interval(metrics_fixed['spec'])\n",
    "            \n",
    "            feature_results[model_name] = {\n",
    "                'metrics_with_fs': metrics_with_fs,\n",
    "                'metrics_fixed': metrics_fixed,\n",
    "                'stable_features': stable_features,\n",
    "                'feature_frequencies': feature_frequencies,\n",
    "                'auc_mean': auc_mean,\n",
    "                'auc_ci': (auc_ci_lower, auc_ci_upper),\n",
    "                'sens_mean': sens_mean,\n",
    "                'sens_ci': (sens_ci_lower, sens_ci_upper),\n",
    "                'spec_mean': spec_mean,\n",
    "                'spec_ci': (spec_ci_lower, spec_ci_upper),\n",
    "                'y_true': y_true,\n",
    "                'y_pred_proba': y_pred_proba\n",
    "            }\n",
    "            \n",
    "            print(f\"AUC: {auc_mean:.3f} ({auc_ci_lower:.3f}-{auc_ci_upper:.3f})\")\n",
    "        \n",
    "        if feature_results:\n",
    "            best_model_name = max(feature_results.keys(), \n",
    "                                key=lambda x: feature_results[x]['auc_mean'])\n",
    "            results[feature_name] = feature_results[best_model_name]\n",
    "            results[feature_name]['best_model_name'] = best_model_name\n",
    "            \n",
    "            prediction_data[feature_name] = {\n",
    "                'y_true': feature_results[best_model_name]['y_true'],\n",
    "                'y_pred_proba': feature_results[best_model_name]['y_pred_proba']\n",
    "            }\n",
    "            \n",
    "            feature_stability_results[feature_name] = {\n",
    "                'feature_frequencies': feature_results[best_model_name]['feature_frequencies'],\n",
    "                'stable_features': feature_results[best_model_name]['stable_features']\n",
    "            }\n",
    "            \n",
    "            print(f\"Best model: {best_model_name} (AUC: {feature_results[best_model_name]['auc_mean']:.3f})\")\n",
    "    \n",
    "    # Perform statistical tests\n",
    "    p_values, best_feature = perform_statistical_tests(results)\n",
    "    \n",
    "    stable_features_data = save_stable_features_to_csv(feature_stability_results)\n",
    "    \n",
    "    return results, feature_stability_results, stable_features_data, p_values, best_feature\n",
    "\n",
    "def plot_feature_stability(feature_stability_results, stable_features_data):\n",
    "    \"\"\"Plot feature stability analysis\"\"\"\n",
    "    print(\"Plotting feature stability...\")\n",
    "    \n",
    "    figs = []\n",
    "    \n",
    "    for feature_name, stability_info in feature_stability_results.items():\n",
    "        feature_frequencies = stability_info['feature_frequencies']\n",
    "        stable_features = stability_info['stable_features']\n",
    "        \n",
    "        freq_df = stable_features_data[feature_name]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        bars = plt.bar(range(len(freq_df)), freq_df['Frequency'])\n",
    "        \n",
    "        for i, (_, row) in enumerate(freq_df.iterrows()):\n",
    "            if row['Stable']:\n",
    "                bars[i].set_color('red')\n",
    "            else:\n",
    "                bars[i].set_color('lightblue')\n",
    "        \n",
    "        plt.axhline(y=FEATURE_FREQ_THRESHOLD, color='r', linestyle='--', \n",
    "                   label=f'Stability threshold ({FEATURE_FREQ_THRESHOLD*100}%)')\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Selection Frequency')\n",
    "        plt.title(f'Feature Stability Analysis - {feature_name}\\n({len(stable_features)} stable features)')\n",
    "        plt.xticks(range(len(freq_df)), freq_df['Feature'], rotation=90)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        figs.append((f\"Feature_Stability_{feature_name}\", plt.gcf()))\n",
    "    \n",
    "    return figs\n",
    "\n",
    "def plot_performance_distribution(results):\n",
    "    \"\"\"Plot performance distribution across feature combinations\"\"\"\n",
    "    print(\"Plotting performance distribution...\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    feature_names = list(results.keys())\n",
    "    auc_data = [results[name]['metrics_fixed']['auc'] for name in feature_names]\n",
    "    \n",
    "    box_plot = plt.boxplot(auc_data, labels=feature_names, patch_artist=True)\n",
    "    \n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "    for patch, color in zip(box_plot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('Performance Distribution Across Feature Combinations')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def plot_statistical_comparison(p_values, feature_names):\n",
    "    \"\"\"Plot heatmap of statistical comparisons\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    mask = np.eye(len(feature_names), dtype=bool)\n",
    "    p_values_masked = np.ma.array(p_values, mask=mask)\n",
    "    \n",
    "    sns.heatmap(p_values, \n",
    "                xticklabels=feature_names, \n",
    "                yticklabels=feature_names,\n",
    "                annot=True, \n",
    "                fmt=\".4f\",\n",
    "                cmap=\"RdYlBu_r\",\n",
    "                center=0.05,\n",
    "                cbar_kws={'label': 'p-value'})\n",
    "    \n",
    "    plt.title('Statistical Comparison of Feature Combinations\\n(Paired t-tests on AUC scores)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def create_shap_plots(results, feature_combinations, y):\n",
    "    \"\"\"Create SHAP plots for model interpretation\"\"\"\n",
    "    print(\"Creating SHAP plots...\")\n",
    "    \n",
    "    shap_figures = []\n",
    "    \n",
    "    for feature_name, result in results.items():\n",
    "        model_name = result['best_model_name']\n",
    "        stable_features = result['stable_features']\n",
    "        \n",
    "        features_df = feature_combinations[feature_name]\n",
    "        \n",
    "        if stable_features:\n",
    "            # Filter features to only stable ones\n",
    "            available_stable = [f for f in stable_features if f in features_df.columns]\n",
    "            features_df = features_df[available_stable]\n",
    "        \n",
    "        X = features_df.values\n",
    "        feature_names = features_df.columns.tolist()\n",
    "        \n",
    "        print(f\"Training final model for SHAP explanation: {feature_name}\")\n",
    "        \n",
    "        models_config = get_model_configurations()\n",
    "        model_config = models_config[model_name]\n",
    "        \n",
    "        X_final, y_final = X, y\n",
    "        \n",
    "        if len(model_config['params']) > 0:\n",
    "            if model_config['use_random_search']:\n",
    "                search = RandomizedSearchCV(\n",
    "                    model_config['model'], \n",
    "                    model_config['params'], \n",
    "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE), \n",
    "                    scoring='roc_auc', \n",
    "                    n_jobs=-1,\n",
    "                    n_iter=RANDOM_SEARCH_ITER,\n",
    "                    random_state=RANDOM_STATE\n",
    "                )\n",
    "            else:\n",
    "                search = GridSearchCV(\n",
    "                    model_config['model'], \n",
    "                    model_config['params'], \n",
    "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE), \n",
    "                    scoring='roc_auc', \n",
    "                    n_jobs=-1\n",
    "                )\n",
    "            \n",
    "            search.fit(X_final, y_final)\n",
    "            final_model = search.best_estimator_\n",
    "        else:\n",
    "            final_model = copy.deepcopy(model_config['model'])\n",
    "            final_model.fit(X_final, y_final)\n",
    "        \n",
    "        try:\n",
    "            model_type = type(final_model).__name__\n",
    "            \n",
    "            if len(X_final) > SHAP_BACKGROUND_SAMPLES:\n",
    "                background_data = shap.sample(X_final, SHAP_BACKGROUND_SAMPLES)\n",
    "            else:\n",
    "                background_data = X_final\n",
    "            \n",
    "            if model_type in ['LogisticRegression']:\n",
    "                explainer = shap.LinearExplainer(final_model, X_final)\n",
    "                shap_values = explainer.shap_values(X_final)\n",
    "                \n",
    "            elif model_type in ['DecisionTreeClassifier', 'RandomForestClassifier', \n",
    "                              'XGBClassifier']:\n",
    "                explainer = shap.TreeExplainer(final_model)\n",
    "                shap_values = explainer.shap_values(X_final)\n",
    "                if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "                    shap_values = shap_values[1]\n",
    "                \n",
    "            else:\n",
    "                def model_predict(x):\n",
    "                    return final_model.predict_proba(x)\n",
    "                \n",
    "                explainer = shap.KernelExplainer(model_predict, background_data)\n",
    "                shap_values = explainer.shap_values(X_final, nsamples=SHAP_NSAMPLES)\n",
    "                \n",
    "                if isinstance(shap_values, list):\n",
    "                    shap_values = np.array(shap_values)\n",
    "                \n",
    "                if len(shap_values.shape) == 3:\n",
    "                    shap_values = shap_values[:, :, 1]\n",
    "            \n",
    "            if len(shap_values.shape) > 2:\n",
    "                shap_values = shap_values.reshape(shap_values.shape[0], -1)\n",
    "            \n",
    "            if shap_values.shape != X_final.shape:\n",
    "                min_features = min(shap_values.shape[1], X_final.shape[1])\n",
    "                shap_values = shap_values[:, :min_features]\n",
    "                X_final_display = X_final[:, :min_features]\n",
    "                feature_names_display = feature_names[:min_features]\n",
    "            else:\n",
    "                X_final_display = X_final\n",
    "                feature_names_display = feature_names\n",
    "            \n",
    "            plot_title = f'{model_name} - {feature_name}\\n({len(stable_features)} stable features)'\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.summary_plot(shap_values, X_final_display, feature_names=feature_names_display, \n",
    "                            plot_type=\"bar\", show=False, max_display=min(15, len(stable_features)))\n",
    "            plt.title(f'SHAP Feature Importance - {plot_title}', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            shap_figures.append((f\"SHAP_Bar_{feature_name}\", plt.gcf()))\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.summary_plot(shap_values, X_final_display, feature_names=feature_names_display, \n",
    "                            show=False, max_display=min(15, len(stable_features)))\n",
    "            plt.title(f'SHAP Summary - {plot_title}', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            shap_figures.append((f\"SHAP_Beeswarm_{feature_name}\", plt.gcf()))\n",
    "            \n",
    "            print(f\"SHAP plots created for {feature_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating SHAP plots for {feature_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return shap_figures\n",
    "\n",
    "def print_detailed_results(results, p_values, best_feature):\n",
    "    \"\"\"Print detailed results summary\"\"\"\n",
    "    print(\"\\nDETAILED RESULTS SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    header = f\"{'Feature Combination':<25} {'Algorithm':<20} {'Stable Features':<15} {'AUC (95% CI)':<20} {'Sensitivity (95% CI)':<25} {'Specificity (95% CI)':<25}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    for feature_name, result in results.items():\n",
    "        model_name = result['best_model_name']\n",
    "        n_stable_features = len(result['stable_features'])\n",
    "        \n",
    "        auc_str = f\"{result['auc_mean']:.3f} ({result['auc_ci'][0]:.3f}-{result['auc_ci'][1]:.3f})\"\n",
    "        sens_str = f\"{result['sens_mean']:.3f} ({result['sens_ci'][0]:.3f}-{result['sens_ci'][1]:.3f})\"\n",
    "        spec_str = f\"{result['spec_mean']:.3f} ({result['spec_ci'][0]:.3f}-{result['spec_ci'][1]:.3f})\"\n",
    "        \n",
    "        marker = \" *\" if feature_name == best_feature else \"\"\n",
    "        row = f\"{feature_name:<25} {model_name:<20} {n_stable_features:<15} {auc_str:<20} {sens_str:<25} {spec_str:<25}{marker}\"\n",
    "        print(row)\n",
    "    \n",
    "    print(f\"\\n* Best performing feature combination\")\n",
    "\n",
    "def save_results_to_csv(results, p_values, best_feature):\n",
    "    \"\"\"Save results to CSV files\"\"\"\n",
    "    print(\"Saving results to CSV files...\")\n",
    "    \n",
    "    # Create results table\n",
    "    table_data = []\n",
    "    \n",
    "    for feature_name, result in results.items():\n",
    "        model_name = result['best_model_name']\n",
    "        n_stable_features = len(result['stable_features'])\n",
    "        \n",
    "        auc_str = f\"{result['auc_mean']:.3f} ({result['auc_ci'][0]:.3f}-{result['auc_ci'][1]:.3f})\"\n",
    "        sens_str = f\"{result['sens_mean']:.3f} ({result['sens_ci'][0]:.3f}-{result['sens_ci'][1]:.3f})\"\n",
    "        spec_str = f\"{result['spec_mean']:.3f} ({result['spec_ci'][0]:.3f}-{result['spec_ci'][1]:.3f})\"\n",
    "        \n",
    "        is_best = feature_name == best_feature\n",
    "        \n",
    "        table_data.append({\n",
    "            'Feature_Combination': feature_name,\n",
    "            'Algorithm': model_name,\n",
    "            'Stable_Features': n_stable_features,\n",
    "            'AUC': result['auc_mean'],\n",
    "            'AUC_CI_Lower': result['auc_ci'][0],\n",
    "            'AUC_CI_Upper': result['auc_ci'][1],\n",
    "            'AUC_95_CI': auc_str,\n",
    "            'Sensitivity': result['sens_mean'],\n",
    "            'Sensitivity_CI_Lower': result['sens_ci'][0],\n",
    "            'Sensitivity_CI_Upper': result['sens_ci'][1],\n",
    "            'Sensitivity_95_CI': sens_str,\n",
    "            'Specificity': result['spec_mean'],\n",
    "            'Specificity_CI_Lower': result['spec_ci'][0],\n",
    "            'Specificity_CI_Upper': result['spec_ci'][1],\n",
    "            'Specificity_95_CI': spec_str,\n",
    "            'Is_Best': is_best\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    results_df = pd.DataFrame(table_data)\n",
    "    results_csv_path = './results/nested_cv_results.csv'\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "    print(f\"Saved results to {results_csv_path}\")\n",
    "    \n",
    "    # Save statistical comparison matrix\n",
    "    feature_names = list(results.keys())\n",
    "    p_values_df = pd.DataFrame(p_values, index=feature_names, columns=feature_names)\n",
    "    p_values_csv_path = './results/nested_cv_statistical_comparison.csv'\n",
    "    p_values_df.to_csv(p_values_csv_path)\n",
    "    print(f\"Saved statistical comparison to {p_values_csv_path}\")\n",
    "    \n",
    "    return results_df, p_values_df\n",
    "\n",
    "def save_figures_to_png(performance_fig, stability_figs, shap_figures, stats_fig):\n",
    "    \"\"\"Save all figures as PNG files to figures folder\"\"\"\n",
    "    print(\"Saving figures as PNG files...\")\n",
    "    \n",
    "    # Create figures directory if it doesn't exist\n",
    "    os.makedirs('./figures', exist_ok=True)\n",
    "    \n",
    "    # Save performance distribution\n",
    "    performance_path = './figures/performance_distribution.png'\n",
    "    performance_fig.savefig(performance_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(performance_fig)\n",
    "    print(f\"Saved performance distribution as {performance_path}\")\n",
    "    \n",
    "    # Save statistical comparison\n",
    "    stats_path = './figures/statistical_comparison.png'\n",
    "    stats_fig.savefig(stats_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(stats_fig)\n",
    "    print(f\"Saved statistical comparison as {stats_path}\")\n",
    "    \n",
    "    # Save stability figures\n",
    "    for name, fig in stability_figs:\n",
    "        safe_name = name.replace('+', '_').replace(' ', '_')\n",
    "        stability_path = f'./figures/{safe_name}.png'\n",
    "        fig.savefig(stability_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved {name} as {stability_path}\")\n",
    "    \n",
    "    # Save SHAP figures\n",
    "    for name, fig in shap_figures:\n",
    "        safe_name = name.replace('+', '_').replace(' ', '_')\n",
    "        shap_path = f'./figures/{safe_name}.png'\n",
    "        fig.savefig(shap_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved {name} as {shap_path}\")\n",
    "    \n",
    "    print(\"All figures saved successfully to ./figures/ folder!\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the complete analysis\"\"\"\n",
    "    try:\n",
    "        # Create output directories if they don't exist\n",
    "        os.makedirs('./results', exist_ok=True)\n",
    "        os.makedirs('./figures', exist_ok=True)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"MACHINE LEARNING ANALYSIS FOR RECURRENCE PREDICTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Load and preprocess data\n",
    "        all_features, y, clinical_end, patho_end, radio_end = load_and_preprocess_data()\n",
    "        \n",
    "        # Step 2: Identify feature types\n",
    "        categorical_features, numerical_features = identify_feature_types(all_features)\n",
    "        \n",
    "        # Step 3: Handle missing values\n",
    "        all_features = handle_missing_values(all_features, categorical_features, numerical_features)\n",
    "        \n",
    "        # Step 4: Prepare feature combinations with PCA\n",
    "        feature_combinations, pca_model, pca_scaler, clinical_features, patho_features, radio_features = prepare_feature_combinations_with_pca(\n",
    "            all_features, clinical_end, patho_end, radio_end\n",
    "        )\n",
    "        \n",
    "        # Step 5: Scale features\n",
    "        scaled_feature_combinations = scale_features(feature_combinations, numerical_features)\n",
    "        \n",
    "        # Step 6: Train and evaluate with nested cross-validation\n",
    "        results, feature_stability_results, stable_features_data, p_values, best_feature = train_and_evaluate_with_nested_cv(\n",
    "            scaled_feature_combinations, y\n",
    "        )\n",
    "        \n",
    "        # Step 7: Create visualizations\n",
    "        print(\"\\nCreating visualizations...\")\n",
    "        performance_fig = plot_performance_distribution(results)\n",
    "        \n",
    "        # Add statistical comparison plot\n",
    "        feature_names = list(results.keys())\n",
    "        stats_fig = plot_statistical_comparison(p_values, feature_names)\n",
    "        \n",
    "        stability_figs = plot_feature_stability(feature_stability_results, stable_features_data)\n",
    "        \n",
    "        shap_figures = create_shap_plots(results, scaled_feature_combinations, y)\n",
    "        \n",
    "        # Step 8: Print detailed results\n",
    "        print_detailed_results(results, p_values, best_feature)\n",
    "        \n",
    "        # Step 9: Save results to CSV\n",
    "        save_results_to_csv(results, p_values, best_feature)\n",
    "        \n",
    "        # Step 10: Save figures to PNG\n",
    "        save_figures_to_png(performance_fig, stability_figs, shap_figures, stats_fig)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Results saved to: ./results/\")\n",
    "        print(f\"Figures saved to: ./figures/\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radiomics",
   "language": "python",
   "name": "rad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
